# Transformers Self-Attention Block

## Overview

This repository contains a practical implementation of the self-attention block commonly used in Transformer models. Self-attention is a key mechanism in Transformers that allows the model to weigh different parts of the input sequence differently when making predictions.

